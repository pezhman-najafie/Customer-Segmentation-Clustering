# -*- coding: utf-8 -*-
"""tamrintz6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16Y8ezpKoF2GrQ69T6t9ICiCueECGIBOp
"""

#Import libraries
#loading dataset
import pandas as pd
import numpy as np

#visualisation
import matplotlib.pyplot as plt
import seaborn as sns
#from kneed import KneeLocator
#!pip install kneed

# data modeling
from sklearn.cluster import KMeans
from sklearn.cluster import AgglomerativeClustering
from sklearn.cluster import MiniBatchKMeans


# Model performance
from sklearn.preprocessing import StandardScaler
from sklearn import metrics
from sklearn.metrics import silhouette_score
from sklearn.metrics import calinski_harabasz_score
from kneed import KneeLocator

#warnings
import warnings
warnings.simplefilter(action='ignore')

Data = pd.read_csv('Customer_Data.csv')

# Preview the dataset
df = pd.DataFrame(Data)
df

# Find the total number of missing values in the dataframe
df.isnull().sum()

df.dropna(inplace = True)

# Drop CUST_ID column
df.drop(['CUST_ID'], axis=1, inplace=True)

# Print the shape of the DataFrame
print(df.shape)

#to view some basic statistical details of train data
df.describe()

plt.figure(figsize=(10,8))
sns.heatmap(df.corr(method='spearman'), square=True, cmap='coolwarm', annot=True)

scaler = StandardScaler()
# fit and transform
df_scale = scaler.fit_transform(df)
# Create a DataFrame from the scaled features
df_scale = pd.DataFrame(df_scale)
df_scale.shape

df_scale1 = pd.DataFrame(df_scale)
df_scale1.columns = ['BALANCE',	'BALANCE_FREQUENCY', 'PURCHASES',	'ONEOFF_PURCHASES',	'INSTALLMENTS_PURCHASES',	'CASH_ADVANCE',	'PURCHASES_FREQUENCY',	'ONEOFF_PURCHASES_FREQUENCY',	'PURCHASES_INSTALLMENTS_FREQUENCY',	'CASH_ADVANCE_FREQUENCY',	'CASH_ADVANCE_TRX',	'PURCHASES_TRX',	'CREDIT_LIMIT',	'PAYMENTS',	'MINIMUM_PAYMENTS',	'PRC_FULL_PAYMENT',	'TENURE']
df_scale1.head()

"""K-Means

Silhouette Coefficient Method:
"""

# Instantiate the KMeans for 4 clusters
km = KMeans(n_clusters=4, random_state=42)
# Fit the KMeans model
km.fit_predict(df_scale1)
# Calculate Silhoutte Score
score = silhouette_score(df_scale1, km.labels_, metric='euclidean')
# Print the score
print('Silhouetter Average Score: %.3f' % score)

#Single Linkage
model = AgglomerativeClustering(n_clusters=4, linkage='single')
labels = model.fit_predict(df_scale1)

silhouette_avg = silhouette_score(df_scale1, labels)
print("Silhouette Score:", silhouette_avg)

"""Optional Section

Elbow Method:The elbow method is a technique used to determine the ideal number of clusters in K-means clustering. This method involves plotting the cost function's value produced by various values of K. The diagram below illustrates how the elbow method operates.

As we can observe from the plot, the average distortion reduces as K increases. This indicates that each cluster will have a smaller number of constituent instances, and these instances will be closer to their respective centroids. However, as K increases, the improvement in average distortion decreases. The point at which the improvement in distortion declines the most is known as the elbow point. This is the optimal number of clusters at which we should stop dividing the data into further clusters.
"""

kmeans_set={"init":"random","n_init":10,"max_iter":300,"random_state":42}

cluster_range = range( 2, 10 )
cluster_errors=[]
for k in cluster_range:
    kmeans= KMeans(n_clusters=k, **kmeans_set) #** open dictionry
    kmeans.fit(df_scale1)
    cluster_errors.append( kmeans.inertia_)

plt.style.use("fivethirtyeight")
plt.plot(range(2,10),cluster_errors)
plt.xticks(range(2,10))
plt.title('Elbow Method For Optimal k')
plt.xlabel('number of clusters')
plt.ylabel('inertia')
plt.show()



k1=KneeLocator(range(2,10),cluster_errors , curve='convex', direction= 'decreasing')
k1.elbow

plt.style.use("fivethirtyeight")
plt.plot(range(2,10),cluster_errors)
plt.xticks(range(2,10))
plt.xlabel('number of clusters')
plt.ylabel('List')
plt.title('Elbow Method For Optimal k')
plt.axvline(x=k1.elbow, color='b', label= 'axvline-full height', ls= '--')
plt.show()

"""Heirarchichal Method:"""

# Calculate Calinski-Harabasz score for different numbers of clusters
calinski_harabaz = []
for n_clust in range(2, 11):
    AgglomerativeC = AgglomerativeClustering(n_clusters=n_clust, affinity='euclidean', linkage='ward')
    AgglomerativeC.fit(df_scale1)
    score = metrics.calinski_harabasz_score(df_scale1, AgglomerativeC.labels_)
    calinski_harabaz.append(score)

# Plot the Calinski-Harabasz score for different numbers of clusters
with plt.style.context("fivethirtyeight"):
    fig, ax = plt.subplots()
    ax.plot(range(2, 11), calinski_harabaz, marker='^', c='b', ms=9, mfc='r')
    ax.set_xticks(range(2, 11))
    ax.set_xlabel("Number of Clusters")
    ax.set_ylabel("Calinski-Harabasz Score")
    ax.set_title("Calinski-Harabasz Plot")
    plt.show()

# Calinski-Harabasz score for different numbers of clusters
calinski_harabaz_scores = []
for k in range(2, 11):
    AgglomerativeC = AgglomerativeClustering(n_clusters=k, affinity='euclidean', linkage='ward')
    labels = AgglomerativeC.fit_predict(df_scale1)
    calinski_harabaz_score_k = calinski_harabasz_score(df_scale1, labels)
    calinski_harabaz_scores.append(calinski_harabaz_score_k)

#Create a dataframe to store the scores
scores_Agg = pd.DataFrame({'k': range(2,11),
'Calinski-Harabasz Score': calinski_harabaz_scores})

print("\nScores for Different Numbers of Clusters:")
scores_Agg.style.background_gradient().set_properties(**{'font-family': 'Segoe UI'}).set_table_styles([{'selector': 'tr:hover', 'props': 'background-color: yellow;'}]).set_table_attributes('style="border-collapse: collapse;"')


#Find the best k for each score
best_k_calinski_harabaz = scores_Agg.loc[scores_Agg['Calinski-Harabasz Score'].idxmax(), 'k']

# Best k for each score
print(f"Best k for Calinski-Harabasz Score: {best_k_calinski_harabaz}")